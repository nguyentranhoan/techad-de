# Basic Spark settings
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.jars.ivy=/tmp/.ivy2

# S3A Configuration - Core settings
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=minioadmin
spark.hadoop.fs.s3a.secret.key=minioadmin
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled=false

# S3A Configuration - All timeout values in milliseconds (CRITICAL!)
spark.hadoop.fs.s3a.connection.timeout=60000
spark.hadoop.fs.s3a.socket.timeout=60000
spark.hadoop.fs.s3a.request.timeout=60000
spark.hadoop.fs.s3a.connection.establish.timeout=60000
spark.hadoop.fs.s3a.threads.keepalivetime=60000
spark.hadoop.fs.s3a.connection.ttl=60000
spark.hadoop.fs.s3a.multipart.purge.age=86400000

# S3A Configuration - Thread pool settings
spark.hadoop.fs.s3a.threads.max=10
spark.hadoop.fs.s3a.threads.core=5
spark.hadoop.fs.s3a.connection.maximum=10
spark.hadoop.fs.s3a.executor.capacity=48

# S3A Configuration - Performance settings
spark.hadoop.fs.s3a.multipart.size=67108864
spark.hadoop.fs.s3a.multipart.threshold=67108864
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.block.size=67108864
spark.hadoop.fs.s3a.readahead.range=65536

# S3A Configuration - Retry settings
spark.hadoop.fs.s3a.attempts.maximum=3
spark.hadoop.fs.s3a.retry.limit=3

# S3A Configuration - Additional settings
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.list.version=2
spark.hadoop.fs.s3a.directory.marker.retention=keep
spark.hadoop.fs.s3a.bulk.delete.page.size=250
spark.hadoop.fs.s3a.change.detection.version.required=false
spark.hadoop.fs.s3a.change.detection.mode=none
spark.hadoop.fs.s3a.committer.magic.enabled=false
spark.hadoop.fs.s3a.committer.name=file
spark.hadoop.fs.s3a.committer.staging.tmp.path=/tmp/staging
spark.hadoop.fs.s3a.signing-algorithm=S3SignerType
spark.hadoop.fs.s3a.server-side-encryption.enabled=false
spark.hadoop.fs.s3a.multipart.uploads.enabled=true
spark.hadoop.fs.s3a.assumed.role.sts.endpoint.region=us-east-1
