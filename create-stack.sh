#!/bin/bash
echo "ğŸ”„ Creating p39-sde-exercise docker-based resources - Airflow, MinIO and Spark containers"

# Safe project-scoped cleanup
echo "ğŸ§¹ First - cleaning up only this project's Docker Compose resources..."
docker compose down --volumes --remove-orphans --rmi all

# Optionally, remove named volumes/networks if you know they are only used by this project:
docker volume rm minio_data postgres_data 2>/dev/null || true
docker network rm spark-net 2>/dev/null || true

# Recreate containers
# Initialize Airflow DB (safe to run multiple times)
echo "ğŸ—„ï¸  Initializing Airflow database..."
docker compose up airflow-init

echo "ğŸš€ Starting containers with new configuration..."
docker compose up -d

# Wait for containers to be ready

echo "â³ Waiting for Spark master to be ready..."
for i in {1..30}; do
  if docker exec spark-master curl -sf http://localhost:8080 > /dev/null; then
    echo "âœ… Spark master is ready!"
    break
  else
    echo "Waiting for Spark master... ($i/30)"
    sleep 2
  fi
done

# Check container status
echo "ğŸ“Š Container status:"
docker compose ps

# Validate MinIO
echo "ğŸ” Checking MinIO health..."
if docker exec minio curl -f http://localhost:9000/minio/health/live > /dev/null 2>&1; then
    echo "âœ… MinIO is ready"
else
    echo "âŒ MinIO not ready"
fi

# Validate Spark master
echo "ğŸ” Checking Spark master..."
if docker exec spark-master curl -f http://localhost:8080 > /dev/null 2>&1; then
    echo "âœ… Spark master is ready"
else
    echo "âŒ Spark master not ready - checking logs..."
    docker logs spark-master | tail -10
fi

# Check basic Spark functionality
echo "ğŸ” Testing basic Spark functionality..."
if docker exec spark-master spark-submit --version > /dev/null 2>&1; then
    echo "âœ… Spark submit is working"
else
    echo "âŒ Spark submit not working"
fi

# Check MinIO bucket and file - FIXED: Use correct alias
echo "ğŸ” Checking MinIO bucket and data..."
# First check if minio-init container completed successfully
if docker ps -a | grep minio-init | grep -q "Exited (0)"; then
    echo "âœ… MinIO initialization completed successfully"
    
    # Check if we can access the bucket contents
    if docker exec minio curl -s http://localhost:9000/sample-bucket/ > /dev/null 2>&1; then
        echo "âœ… Sample bucket is accessible"
    else
        echo "âŒ Sample bucket not accessible"
    fi
else
    echo "âŒ MinIO initialization failed - checking logs..."
    docker logs minio-init | tail -10
fi

# Check if jobs directory exists
echo "ğŸ” Checking jobs directory..."
if docker exec spark-master ls -la /opt/bitnami/spark/jobs/ > /dev/null 2>&1; then
    echo "âœ… Jobs directory is mounted"
    docker exec spark-master ls -la /opt/bitnami/spark/jobs/
else
    echo "âŒ Jobs directory not mounted"
fi

# Check if JAR files are properly installed
echo "ğŸ” Checking S3A JAR files..."
if docker exec spark-master ls -la /opt/bitnami/spark/jars/ | grep -q "hadoop-aws-3.3.4.jar"; then
    echo "âœ… Hadoop AWS JAR is present"
else
    echo "âŒ Hadoop AWS JAR missing"
fi

if docker exec spark-master ls -la /opt/bitnami/spark/jars/ | grep -q "aws-java-sdk-bundle-1.12.470.jar"; then
    echo "âœ… AWS SDK JAR is present"
else
    echo "âŒ AWS SDK JAR missing"
fi

# Check configuration files
echo "ğŸ” Checking configuration files..."
if docker exec spark-master ls -la /opt/bitnami/spark/conf/spark-defaults.conf > /dev/null 2>&1; then
    echo "âœ… spark-defaults.conf is present"
else
    echo "âŒ spark-defaults.conf missing"
fi

if docker exec spark-master ls -la /opt/bitnami/spark/conf/core-site.xml > /dev/null 2>&1; then
    echo "âœ… core-site.xml is present"
else
    echo "âŒ core-site.xml missing"
fi

# Test S3A connectivity
echo "ğŸ” Testing S3A connectivity..."
if docker exec spark-master python3 -c "
from pyspark.sql import SparkSession
try:
    spark = SparkSession.builder.appName('ConnectivityTest').getOrCreate()
    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
    fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(
        spark.sparkContext._jvm.java.net.URI.create('s3a://sample-bucket/'),
        hadoop_conf
    )
    print('âœ… S3A filesystem connectivity successful')
    spark.stop()
except Exception as e:
    import traceback; traceback.print_exc()
" > /dev/null 2>&1; then
    echo "âœ… S3A connectivity test passed"
else
    echo "âŒ S3A connectivity test failed"
fi

echo ""
echo "ğŸ‰ Setup complete!"
echo ""
echo "ğŸ“‹ Summary:"
echo "  - Using pre-installed JAR approach for S3A dependencies"
echo "  - JAR files are installed during container initialization"
echo "  - Configuration is handled via spark-defaults.conf and core-site.xml"
echo ""
echo "ğŸš€ To run your job (simple approach):"
echo "docker exec spark-master python3 /opt/bitnami/spark/jobs/read_s3_csv.py"
echo ""
echo "ğŸš€ To run your job (spark-submit approach):"
echo "docker exec spark-master spark-submit \\"
echo "  --master spark://spark-master:7077 \\"
echo "  /opt/bitnami/spark/jobs/sample_job.py"
echo ""
echo "ğŸŒ Available services:"
echo "  - Spark Master UI: http://localhost:8080"
echo "  - MinIO Console: http://localhost:9001 (minioadmin/minioadmin)"
echo "  - Airflow UI: http://localhost:8081 (admin/admin)"
echo ""
echo "ğŸ“ Note: JAR files are pre-installed using the working versions:"
echo "  - hadoop-aws-3.3.4.jar (compatible with AWS SDK v1)"
echo "  - aws-java-sdk-bundle-1.12.470.jar (AWS SDK v1)"
echo "  This avoids the ClassNotFoundException we encountered earlier."
echo "âš ï¸  Note: Airflow may take 3â€“5 minutes to become accessible after stack creation, especially on the first run."
echo "    You can check progress with: docker compose logs -f airflow-webserver"